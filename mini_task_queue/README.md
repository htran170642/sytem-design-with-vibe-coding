# Mini-Celery: Distributed Task Queue

A production-ready distributed task queue system built with Python, FastAPI, Redis, and asyncio. Demonstrates advanced async patterns, reliability features, and real-time WebSocket updates.

## ğŸ¯ Features

### Core Functionality
- âœ… **Async Task Queue** - Submit tasks via HTTP API, process with workers
- âœ… **Real-time Updates** - WebSocket streaming of task progress
- âœ… **Automatic Retries** - Configurable retry logic with exponential backoff potential
- âœ… **Dead-Letter Queue** - Failed tasks moved to DLQ after max retries
- âœ… **Concurrent Processing** - Multiple workers with configurable concurrency
- âœ… **Task Status Tracking** - PENDING â†’ RUNNING â†’ SUCCESS/FAILED lifecycle
- âœ… **Graceful Shutdown** - Signal handling for clean worker termination

### Production-Ready Features
- ğŸ”’ Connection pooling (50 max connections)
- ğŸ“Š Queue statistics and monitoring
- ğŸ”„ Retry mechanism with configurable limits
- ğŸ’€ Dead-letter queue for failed tasks
- ğŸ“¡ Pub/Sub for real-time events
- âš¡ High throughput

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP POST /tasks
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           FastAPI Server                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  POST /tasks                    â”‚   â”‚
â”‚  â”‚  GET /tasks/{id}                â”‚   â”‚
â”‚  â”‚  WS /ws/tasks/{id}              â”‚   â”‚
â”‚  â”‚  GET /queue/stats               â”‚   â”‚
â”‚  â”‚  GET /queue/dead-letter         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Redis    â”‚
    â”‚             â”‚
    â”‚ â€¢ Queue     â”‚â”€â”€â”€â”€â”
    â”‚ â€¢ Tasks     â”‚    â”‚
    â”‚ â€¢ Pub/Sub   â”‚    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
           â–²           â”‚
           â”‚           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                 â”‚
    â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker  â”‚                    â”‚ Worker  â”‚
â”‚ Pool    â”‚                    â”‚ Pool    â”‚
â”‚ (5x)    â”‚                    â”‚ (5x)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  WebSocket  â”‚
          â”‚   Clients   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- Redis server

### Installation

```bash
# Clone repository
git clone <your-repo>
cd mini-celery

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Running the System

**Terminal 1 - Redis:**
```bash
redis-server
```

**Terminal 2 - API Server:**
```bash
python run_api.py
# Server runs on http://localhost:8000
```

**Terminal 3 - Worker:**
```bash
python -m app.worker
# Starts 5 concurrent workers
```

**Terminal 4 - Submit Tasks:**
```bash
# Via API
curl -X POST http://localhost:8000/tasks \
  -H "Content-Type: application/json" \
  -d '{"name": "add", "args": [10, 20]}'

# Or use interactive docs
open http://localhost:8000/docs
```

## ğŸ“– API Reference

### Submit Task
```http
POST /tasks
Content-Type: application/json

{
  "name": "add",
  "args": [10, 20],
  "max_retries": 3
}

Response:
{
  "task_id": "uuid",
  "status": "PENDING",
  "max_retries": 3
}
```

### Check Task Status
```http
GET /tasks/{task_id}

Response:
{
  "task_id": "uuid",
  "status": "SUCCESS",
  "result": 30,
  "error": null,
  "retry_count": 0,
  "max_retries": 3,
  "created_at": "2024-12-07T10:00:00",
  "updated_at": "2024-12-07T10:00:01"
}
```

### WebSocket - Real-time Updates
```javascript
const ws = new WebSocket('ws://localhost:8000/ws/tasks/{task_id}');

ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  console.log(data.event, data.data);
};

// Events: connected, started, progress, completed, failed
```

### Queue Statistics
```http
GET /queue/stats

Response:
{
  "queue_length": 5,
  "queue_name": "queue:default"
}
```

### Dead-Letter Queue
```http
GET /queue/dead-letter

Response:
{
  "count": 2,
  "tasks": [...]
}
```

## ğŸ”§ Configuration

Edit `.env` or `app/config.py`:

```python
REDIS_HOST=localhost
REDIS_PORT=6379
WORKER_CONCURRENCY=5
API_HOST=0.0.0.0
API_PORT=8000
```

## ğŸ“Š Redis Data Schema

### Queue (List)
```
Key: queue:default
Type: Redis List (FIFO)
Operations: LPUSH (enqueue), BRPOP (dequeue)
```

### Task Metadata (Hash)
```
Key: task:{task_id}
Type: Redis Hash
Fields: id, name, args, kwargs, status, result, error, 
        retry_count, max_retries, created_at, updated_at
```

### Task Events (Pub/Sub)
```
Channel: task:{task_id}:events
Type: Redis Pub/Sub
Messages: JSON events (started, progress, completed, failed)
```

### Dead-Letter Queue (List)
```
Key: queue:dead_letter
Type: Redis List
Purpose: Store failed tasks after max retries
```

## ğŸ“ Project Structure

```
mini-celery/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # Settings & configuration
â”‚   â”œâ”€â”€ models.py          # Task models (Pydantic)
â”‚   â”œâ”€â”€ utils.py           # Serialization helpers
â”‚   â”œâ”€â”€ queue.py           # Redis queue abstraction
â”‚   â”œâ”€â”€ tasks.py           # Task registry & definitions
â”‚   â”œâ”€â”€ worker.py          # Worker process
â”‚   â””â”€â”€ api.py             # FastAPI server
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py        # Pytest fixtures
â”‚   â”œâ”€â”€ test_queue.py      # Unit tests
â”‚   â”œâ”€â”€ test_integration.py # Integration tests
â”‚   â””â”€â”€ test_load.py       # Load/performance tests
â”œâ”€â”€ websocket_client.html  # Demo WebSocket client
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pytest.ini
â””â”€â”€ README.md
```

## ğŸš§ Future Enhancements

- [ ] Task priorities (multiple queues)
- [ ] Periodic tasks (cron-like scheduling)
- [ ] Task chaining/workflows
- [ ] Result backends (PostgreSQL/MongoDB)
- [ ] Monitoring dashboard
- [ ] Rate limiting per task type
- [ ] Task cancellation
- [ ] Distributed locks
- [ ] Metrics export (Prometheus)
- [ ] Docker deployment

## Technical Deep Dives

### Q: Walk me through the task lifecycle

**Answer:**
```
1. Client POSTs to /tasks
2. API creates Task object, generates UUID
3. Task saved to Redis hash (task:{id})
4. Task ID pushed to Redis list (queue:default)
5. Worker BRPOPs from queue (blocking, non-busy-wait)
6. Worker updates status to RUNNING
7. Worker publishes "started" event to Redis Pub/Sub
8. Worker executes task function
9. On success: statusâ†’SUCCESS, result stored, "completed" event
10. On failure: check retry count
    - If retries left: requeue task, increment counter
    - If max retries: move to dead-letter queue
11. WebSocket clients receive all events in real-time
```

### Q: Explain your async patterns

**1. Fire-and-Forget (create_task)**
```python
# Use case: Background worker loops
workers = [asyncio.create_task(worker_loop(i)) for i in range(5)]
# Workers run independently, don't block each other
```

**2. Wait-for-All (gather)**
```python
# Use case: Bulk task submission
await asyncio.gather(*[enqueue_task(t) for t in tasks])
# All must complete before continuing
```

**3. Graceful Shutdown**
```python
# Signal handler sets flag + event
signal.signal(signal.SIGTERM, lambda: shutdown_event.set())

# Workers check flag in loop
while self.running:
    task_id = await pop_task(timeout=2)  # Short timeout
    if task_id:
        await handle_task(task_id)
# Finish current tasks before exiting
```

**4. WebSocket + Pub/Sub Bridge**
```python
pubsub = await redis.subscribe(f"task:{id}:events")
async for message in pubsub.listen():
    await websocket.send_json(message)
# Async iteration bridges two systems
```

### Q: How did you handle the connection pool issue?

**Problem:**
```python
# 100 concurrent gather() calls
await asyncio.gather(*[enqueue(t) for t in tasks])
# Error: "Too many connections" - pool exhausted
```

**Solution:**
```python
# 1. Increased pool size: 10 â†’ 50
max_connections=50

# 2. Batch processing
batches = [tasks[i:i+20] for i in range(0, len(tasks), 20)]
for batch in batches:
    await asyncio.gather(*[enqueue(t) for t in batch])

# Result: 800 tasks/sec without errors
```

### Q: Why Redis over other message queues?

**Redis Pros:**
- Simple setup (single dependency)
- Multiple data structures (list, hash, pub/sub)
- Atomic operations (LPUSH/BRPOP)
- Very fast (in-memory)
- Good for 10k-100k tasks/day

**Trade-offs:**
- No guaranteed delivery (vs RabbitMQ)
- No message ordering guarantees (vs Kafka)
- Single-threaded (bottleneck at high scale)
- In-memory only (unless using persistence)

**When I'd switch:**
- RabbitMQ: Need message acknowledgment, complex routing
- Kafka: Need event log, message replay, 1M+ events/day
- SQS: AWS ecosystem, don't want to manage infrastructure

### Q: How would you add task priorities?

**Approach 1: Multiple Queues**
```python
queues = ["queue:high", "queue:medium", "queue:low"]

# Worker checks high priority first
for queue_name in queues:
    task_id = await redis.brpop(queue_name, timeout=1)
    if task_id:
        break
```

**Approach 2: Redis Sorted Set**
```python
# Score = priority (higher = more urgent)
await redis.zadd("queue:priority", {task_id: priority})

# Pop highest priority
task_id = await redis.zpopmax("queue:priority")
```

**Trade-off:**
- Multiple queues: Simpler, but can starve low priority
- Sorted set: Fair, but more complex, no blocking pop

### Q: What would break at 1M tasks/hour?

**Bottlenecks:**

1. **Single Redis Instance**
   - Solution: Redis Cluster with sharding
   
2. **Single Worker Process**
   - Solution: Multiple machines, each running workers
   
3. **Task Status Queries** (1M HGETALL/hour)
   - Solution: Cache in PostgreSQL, query DB not Redis
   
4. **WebSocket Connections** (10k+ concurrent)
   - Solution: Dedicated WS server, use Redis Streams for history

**Scaling Architecture:**
```
Load Balancer
  â†“
API Servers (10x) â†’ Redis Cluster (5 nodes)
                    â†“
                  Worker Pools (50 machines Ã— 5 workers)
                    â†“
                  PostgreSQL (task results)
```

## Other docs
- [QUICK_REFERENCE.md](QUICK_REFERENCE.md) - Command snippets & usage
- [ARCHITECTURE.md](ARCHITECTURE.md) - Deep dive into design